{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Onur Can\n#### 1. Importing & Pre-Processing of Data  &  2. Initialization of Data Structures\n#### 3. Tree Inference & 4. Rule Extraction\n#### 5. Training & Test Predictions and RMSE values & 6. Plotting Traning & Test & Fit ---------> ( P = 25 )\n#### 7. Generalization to P = 5,10,15,.............50 and & 8. Final RMSE Plotting","metadata":{}},{"cell_type":"code","source":"# Onur Can \n# Project is done for Prof. Mehmet Gönen's DASC 521: Introduction to Machine Learning @ Koç University MSc Data Science Program\n# Thanks Prof Mehmet for the dataset generation and instructions\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-06-05T15:23:34.689371Z","iopub.execute_input":"2022-06-05T15:23:34.689743Z","iopub.status.idle":"2022-06-05T15:23:34.694617Z","shell.execute_reply.started":"2022-06-05T15:23:34.689712Z","shell.execute_reply":"2022-06-05T15:23:34.693799Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### 1. Importing & Pre-Processing of Data","metadata":{}},{"cell_type":"code","source":"# importing the data set\ndata_set = np.genfromtxt(\"../input/volcan-dataset/volcano_data_set.csv\", delimiter = \",\" , skip_header= 1)\n\n# Splitting test-training data\ntraining_set = data_set[0:150,:]\ntest_set = data_set[150:,:]\nprint(training_set.shape)\nprint(test_set.shape)\n\n#Splitting and preparing the data\nx_train = training_set[:,0]\ny_train = training_set[:,1]\nx_test = test_set[:,0]\ny_test = test_set[:,1]\n\nN_train = len(y_train)\nN_test = len(y_test)\n\nprint(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\nprint(\"min x_train value = \", np.min(x_train), \"max x_train value = \", np.max(x_train))\nprint(\"min x_test value = \", np.min(x_test), \"max x_test value = \", np.max(x_test))","metadata":{"execution":{"iopub.status.busy":"2022-06-05T15:23:34.711403Z","iopub.execute_input":"2022-06-05T15:23:34.711897Z","iopub.status.idle":"2022-06-05T15:23:34.733459Z","shell.execute_reply.started":"2022-06-05T15:23:34.711859Z","shell.execute_reply":"2022-06-05T15:23:34.731662Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### 2. Initialization of Data Structures","metadata":{}},{"cell_type":"code","source":"# create necessary data structures\n\nP = 25 # Pre_Pruning Parameter / will be changed later at PART 6.\n\n# This is for plotting purposes\nlower_bound = +1.50\nupper_bound = +5.2\nlower_bound_interval = +1.5 # interval values will be our prediction boundaries\nupper_bound_interval = +5.2\ndata_interval = np.linspace(lower_bound,upper_bound,371)\n\n# Setting up Dictionaries for nodes data\nnode_indices = {}\nis_terminal = {}\nneed_split = {}\nnode_splits = {}\nnode_frequencies = {}\n# Below two dictionaries were added to to keep track of:\nnode_split_interval = {}    # after splits, keeping the track of split invervals to predict\nnode_averages = {}          # this is the prediction value if new_data in the interval\n\n# put all training instances into the root node\nnode_indices[1] = np.array(range(N_train))\nis_terminal[1] = False\nneed_split[1] = True\n# These two dictionaries will be updated for each node see below\nnode_split_interval[1] = [lower_bound_interval, upper_bound_interval] # initialized via all data_interval\nnode_averages[1] = np.sum(y_train) / N_train # initiliazed via mean","metadata":{"execution":{"iopub.status.busy":"2022-06-05T15:23:34.736073Z","iopub.execute_input":"2022-06-05T15:23:34.736680Z","iopub.status.idle":"2022-06-05T15:23:34.747662Z","shell.execute_reply.started":"2022-06-05T15:23:34.736634Z","shell.execute_reply":"2022-06-05T15:23:34.746754Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### 3. Tree Inference","metadata":{}},{"cell_type":"code","source":"# learning algorithm\nwhile True:\n    # find nodes that need splitting\n    split_nodes = [key for key, value in need_split.items() if value == True]\n    # check whether we reach all terminal nodes\n    # if this is 0 then there is no elements in need_split nodes array. stop algorithm\n    # ie. no nodes requires more splitting\n    if len(split_nodes) == 0:\n        break\n    \n    # find best split positions for all nodes\n    for split_node in split_nodes:\n        data_indices = node_indices[split_node]\n        # the node is either gonna be splitted or a gonna be a terminal node so need_split = False\n        need_split[split_node] = False\n        node_frequencies[split_node] = len(y_train[data_indices])\n        \n        # a node reaches element count < P it will automatically be a terminal node\n        if len(y_train[data_indices]) <= P:\n            is_terminal[split_node] = True\n        # if not terminal condition satisfied then we will split in else statement\n        else:\n            is_terminal[split_node] = False\n            \n            # Preparation for split score calculations\n            unique_values = np.sort(x_train[data_indices])\n            # middle points of consecutive x_data points\n            split_positions = (unique_values[1:len(unique_values)] + unique_values[0:(len(unique_values) - 1)]) / 2\n            split_scores = np.repeat(0.0, len(split_positions))\n            left_branch_averages = np.repeat(0.0, len(split_positions))\n            right_branch_averages = np.repeat(0.0, len(split_positions))\n            \n            for s in range(len(split_positions)):\n                left_indices = data_indices[x_train[data_indices] >= split_positions[s]]\n                right_indices = data_indices[x_train[data_indices] <= split_positions[s]]\n                # For each split calculate the -MEAN SQUARED ERROR-\n                split_scores[s] = ( np.sum((y_train[left_indices] - np.mean(y_train[left_indices]))**2) + np.sum((\n                y_train[right_indices] - np.mean(y_train[right_indices]))**2))  * 1 / len(unique_values)\n                left_branch_averages[s] = np.mean(y_train[left_indices])\n                right_branch_averages[s] = np.mean(y_train[right_indices])\n                \n            # Argmin for minimum error index\n            best_splits = split_positions[np.argmin(split_scores)]\n            # Node averages of Left and Right Child \n            left_branch_average = left_branch_averages[np.argmin(split_scores)]\n            right_branch_average = right_branch_averages[np.argmin(split_scores)]\n\n            # decided where to split along X axis\n            # add it to dictinary with node_split key value\n            node_splits[split_node] = best_splits\n            \n            # create left node using the selected split\n            \n            left_indices = data_indices[x_train[data_indices] >= best_splits]\n            node_indices[2 * split_node] = left_indices\n            is_terminal[2 * split_node] = False\n            need_split[2 * split_node] = True\n            #for any left child upper bound is coming from parent while lower is split_position\n            node_split_interval[2 * split_node] = [best_splits, node_split_interval[split_node][1]]\n            # Update child node averages\n            node_averages[2 * split_node] = left_branch_average\n      \n            # create right node using the selected split\n            right_indices = data_indices[x_train[data_indices] <= best_splits]\n            node_indices[2 * split_node + 1] = right_indices\n            is_terminal[2 * split_node + 1] = False\n            need_split[2 * split_node + 1] = True\n            #for any right child lower bound is coming from parent while upper is split_position\n            node_split_interval[2 * split_node + 1] = [node_split_interval[split_node][0], best_splits]\n            # Update child node averages\n            node_averages[2 * split_node + 1] = right_branch_average\n            \n(print(\"--------------Which Nodes are Terminal -----------\"))            \nprint(is_terminal)\n(print(\"--------------Split Positions of Nodes -----------\"))  \nprint(node_splits)\n(print(\"--------------Node Frequencies -----------\"))  \nprint(node_frequencies)\n(print(\"--------------Split Interval of Each Node -----------\")) \nprint(node_split_interval)\n(print(\"--------------Node Averages -----------\")) \nprint(node_averages)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T15:23:34.749401Z","iopub.execute_input":"2022-06-05T15:23:34.750426Z","iopub.status.idle":"2022-06-05T15:23:34.844465Z","shell.execute_reply.started":"2022-06-05T15:23:34.750375Z","shell.execute_reply":"2022-06-05T15:23:34.843096Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### 4. Rule Extraction","metadata":{}},{"cell_type":"code","source":"# extract rules\nprint(\" When P = \", P, \" ----------------------------------------\" )\nterminal_nodes = [key for key, value in is_terminal.items() if value == True]\nfor terminal_node in terminal_nodes:\n    index = terminal_node\n    rules = np.array([])\n    while index > 1:\n        parent = np.floor(index / 2)\n        if index % 2 == 0:\n            # if node is left child of its parent\n            rules = np.append(rules, \"x > {:.2f}\".format(node_splits[parent]))\n        else:\n            # if node is right child of its parent\n            rules = np.append(rules, \"x <= {:.2f}\".format(node_splits[parent]))\n        index = parent\n    rules = np.flip(rules)\n    print(\" Terminal Node index {} Ruleset --- {} => {} \".format(terminal_node, rules, node_frequencies[terminal_node]))","metadata":{"execution":{"iopub.status.busy":"2022-06-05T15:23:34.847667Z","iopub.execute_input":"2022-06-05T15:23:34.848048Z","iopub.status.idle":"2022-06-05T15:23:34.860010Z","shell.execute_reply.started":"2022-06-05T15:23:34.848015Z","shell.execute_reply":"2022-06-05T15:23:34.858969Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### 5. Training & Test Predictions and RMSE values","metadata":{}},{"cell_type":"code","source":"# We have trained our model with traning data and our tree's splits and \n# interval for splits are determined. Put new x_i's to predict  \n\n#traning set predictions\nterminals =  [key for key, value in is_terminal.items() if value == True]\nx_train_pred = np.empty(N_train)\n# For each x_train data check which interval it fits in terminal nodes intervals dictionary\nfor i in range(len(x_train_pred)):\n    for j in terminals:\n        if node_split_interval[j][0] <= x_train[i] and x_train[i] < node_split_interval[j][1]:\n            x_train_pred[i] = node_averages[j]\n\n#test set predictions\nx_test_pred = np.empty(N_test)\n# For each x_test data check which interval it fits in terminal nodes intervals dictionary\nfor i in range(len(x_test_pred)):\n    for j in terminals:\n        if node_split_interval[j][0] <= x_test[i] and x_test[i] <= node_split_interval[j][1]:\n            x_test_pred[i] = node_averages[j]\n\n# For plotting purposes black line from +1.5 to +5.2 and predict each value\ndata_interval_pred = np.empty(len(data_interval))\nfor i in range(len(data_interval)):\n    for j in terminals:\n        if node_split_interval[j][0] <= data_interval[i] and data_interval[i] <= node_split_interval[j][1]:\n            data_interval_pred[i] = node_averages[j]\n            \nRMSE_training = np.sqrt(np.sum(((x_train_pred - y_train)**2)) / N_train )\nprint(\"--------------------------------------------------------\")\nprint(\"Training Set RMSE = \", RMSE_training,\" when P = \", P)\nprint(\"--------------------------------------------------------\")\nRMSE_test = np.sqrt(np.sum(((x_test_pred - y_test)**2)) / N_test )\nprint(\"--------------------------------------------------------\")\nprint(\"Test Set RMSE = \", RMSE_test,\" when P = \", P)\nprint(\"--------------------------------------------------------\")\n\n\n#print(terminals)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T15:23:34.862323Z","iopub.execute_input":"2022-06-05T15:23:34.862854Z","iopub.status.idle":"2022-06-05T15:23:34.889070Z","shell.execute_reply.started":"2022-06-05T15:23:34.862803Z","shell.execute_reply":"2022-06-05T15:23:34.887987Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### 6. Plotting Traning & Test & Fit ( P = 25 )","metadata":{}},{"cell_type":"code","source":"#Plotting all the fields in the homework\nplt.figure( figsize = (12,6))\nplt.plot(x_train, y_train, \"b.\", markersize = 10, label = \"training\")\nplt.plot(x_test, y_test, \"r.\", markersize = 10, label = \"test\")\nplt.plot(data_interval, data_interval_pred, \"k-\")\nplt.grid()\nplt.xlabel(\"Eruption Time (min)\")\nplt.ylabel(\"Waiting time to next eruption (min)\")\nplt.legend(loc=\"upper left\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T15:23:34.891526Z","iopub.execute_input":"2022-06-05T15:23:34.892564Z","iopub.status.idle":"2022-06-05T15:23:35.152450Z","shell.execute_reply.started":"2022-06-05T15:23:34.892506Z","shell.execute_reply":"2022-06-05T15:23:35.150961Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### 7. Generalization to P = 5,10,15,.............50 and their RMSE Plotting\n##### Defined function is exact copy of Part 3. It is for just to iterate over P values","metadata":{}},{"cell_type":"code","source":"# Set P in wanted range\n# Define arrays for holding parameter values and their Respective RMSE_Train / Test Values\nPrune_parameter = np.arange(5,55,5)\nRMSE_training_values = np.empty(len(Prune_parameter))\nRMSE_test_values = np.empty(len(Prune_parameter))\n\ndef RMSE_by_P( K , set1, set2):\n    x_train_by_P = set1[:,0]\n    y_train_by_P = set1[:,1]\n    x_test_by_P = set2[:,0]\n    y_test_by_P = set2[:,1]\n    N_train_by_P = len(y_train_by_P)\n    N_test_by_P = len(y_test_by_P)\n    Prune = K\n    lower_bound_interval_by_P = +1.5\n    upper_bound_interval_by_P = +5.2\n    node_indices_by_P = {}\n    is_terminal_by_P = {}\n    need_split_by_P = {}\n    node_splits_by_P = {}\n    node_frequencies_by_P = {}\n    node_split_interval_by_P = {}    # after splits, keeping the track of split invervals to predict\n    node_averages_by_P = {} \n    node_indices_by_P[1] = np.array(range(N_train_by_P))\n    is_terminal_by_P[1] = False\n    need_split_by_P[1] = True\n    node_split_interval_by_P[1] = [lower_bound_interval_by_P, upper_bound_interval_by_P]\n    node_averages_by_P[1] = np.sum(y_train_by_P) / N_train_by_P\n    while True:\n        split_nodes_by_P = [key for key, value in need_split_by_P.items() if value == True]\n        if len(split_nodes_by_P) == 0:\n            break\n        for split_node_by_P in split_nodes_by_P:\n            data_indices_by_P = node_indices_by_P[split_node_by_P]\n            need_split_by_P[split_node_by_P] = False\n            node_frequencies_by_P[split_node_by_P] = len(y_train_by_P[data_indices_by_P])\n            if len(y_train_by_P[data_indices_by_P]) <= Prune:\n                is_terminal_by_P[split_node_by_P] = True\n            else:\n                is_terminal_by_P[split_node_by_P] = False\n                unique_values_by_P = np.sort(x_train_by_P[data_indices_by_P])\n                split_positions_by_P = (unique_values_by_P[1:len(unique_values_by_P)] + unique_values_by_P[0:(len(unique_values_by_P) - 1)]) / 2\n                split_scores_by_P = np.repeat(0.0, len(split_positions_by_P))\n                left_branch_averages_by_P = np.repeat(0.0, len(split_positions_by_P))\n                right_branch_averages_by_P = np.repeat(0.0, len(split_positions_by_P))\n                for s in range(len(split_positions_by_P)):\n                    left_indices_by_P = data_indices_by_P[x_train_by_P[data_indices_by_P] >= split_positions_by_P[s]]\n                    right_indices_by_P = data_indices_by_P[x_train_by_P[data_indices_by_P] <= split_positions_by_P[s]]\n                    split_scores_by_P[s] = ( np.sum((y_train_by_P[left_indices_by_P] - np.mean(y_train_by_P[left_indices_by_P]))**2) + np.sum((\n                    y_train_by_P[right_indices_by_P] - np.mean(y_train_by_P[right_indices_by_P]))**2))  * 1 / len(unique_values_by_P)\n                    left_branch_averages_by_P[s] = np.mean(y_train_by_P[left_indices_by_P])\n                    right_branch_averages_by_P[s] = np.mean(y_train_by_P[right_indices_by_P])\n                best_splits_by_P = split_positions_by_P[np.argmin(split_scores_by_P)]\n                left_branch_average_by_P = left_branch_averages_by_P[np.argmin(split_scores_by_P)]\n                right_branch_average_by_P = right_branch_averages_by_P[np.argmin(split_scores_by_P)]\n                node_splits[split_node_by_P] = best_splits_by_P\n                left_indices_by_P = data_indices_by_P[x_train_by_P[data_indices_by_P] >= best_splits_by_P]\n                node_indices_by_P[2 * split_node_by_P] = left_indices_by_P\n                is_terminal_by_P[2 * split_node_by_P] = False\n                need_split_by_P[2 * split_node_by_P] = True\n                node_split_interval_by_P[2 * split_node_by_P] = [best_splits_by_P, node_split_interval_by_P[split_node_by_P][1]]\n                node_averages_by_P[2 * split_node_by_P] = left_branch_average_by_P\n                right_indices_by_P = data_indices_by_P[x_train_by_P[data_indices_by_P] <= best_splits_by_P]\n                node_indices_by_P[2 * split_node_by_P + 1] = right_indices_by_P\n                is_terminal_by_P[2 * split_node_by_P + 1] = False\n                need_split_by_P[2 * split_node_by_P + 1] = True\n                node_split_interval_by_P[2 * split_node_by_P + 1] = [node_split_interval_by_P[split_node_by_P][0], best_splits_by_P]\n                node_averages_by_P[2 * split_node_by_P + 1] = right_branch_average_by_P   \n    terminals_by_P =  [key for key, value in is_terminal_by_P.items() if value == True]\n    x_train_pred_by_P = np.empty(N_train_by_P)\n    for i in range(len(x_train_pred_by_P)):\n        for j in terminals_by_P:\n            if node_split_interval_by_P[j][0] <= x_train_by_P[i] and x_train_by_P[i] < node_split_interval_by_P[j][1]:\n                x_train_pred_by_P[i] = node_averages_by_P[j]\n    x_test_pred_by_P = np.empty(N_test_by_P)\n    for i in range(len(x_test_pred_by_P)):\n        for j in terminals_by_P:\n            if node_split_interval_by_P[j][0] <= x_test_by_P[i] and x_test_by_P[i] <= node_split_interval_by_P[j][1]:\n                x_test_pred_by_P[i] = node_averages_by_P[j]\n    return x_train_pred_by_P, x_test_pred_by_P\n\n#Iterating over P values while calling the function\nfor i in range(len(Prune_parameter)):\n    x_train_pred_by_P, x_test_pred_by_P = RMSE_by_P(Prune_parameter[i], training_set, test_set)\n    RMSE_training_values[i] = np.sqrt(np.sum(((x_train_pred_by_P - y_train)**2)) / N_train )\n    RMSE_test_values[i] =  np.sqrt(np.sum(((x_test_pred_by_P - y_test)**2)) / N_test )\n    print(\"When P = \", Prune_parameter[i], \"--- RMSE_train = \", RMSE_training_values[i] , \"--- RMSE_test = \", RMSE_test_values[i]) ","metadata":{"execution":{"iopub.status.busy":"2022-06-05T15:23:35.154404Z","iopub.execute_input":"2022-06-05T15:23:35.154872Z","iopub.status.idle":"2022-06-05T15:23:35.815419Z","shell.execute_reply.started":"2022-06-05T15:23:35.154825Z","shell.execute_reply":"2022-06-05T15:23:35.814365Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### 8. Final RMSE Plotting","metadata":{}},{"cell_type":"code","source":"#Putting all arrays together\nplt.figure(figsize = (8,8))\nplt.plot(Prune_parameter, RMSE_training_values, \"b.-\", label = \"training\", markersize= 12)\nplt.plot(Prune_parameter, RMSE_test_values, \"r.-\", label = \"test\", markersize= 12)\nplt.xlabel(\"Pre-pruning size(P)\")\nplt.ylabel(\"RMSE\")\nplt.legend(loc=\"upper right\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T15:23:35.816544Z","iopub.execute_input":"2022-06-05T15:23:35.816860Z","iopub.status.idle":"2022-06-05T15:23:36.016560Z","shell.execute_reply.started":"2022-06-05T15:23:35.816832Z","shell.execute_reply":"2022-06-05T15:23:36.015557Z"},"trusted":true},"execution_count":12,"outputs":[]}]}