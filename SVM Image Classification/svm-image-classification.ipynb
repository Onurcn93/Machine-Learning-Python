{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Onur Can\n### 20.12.2021\n#### 1. Importing and Processing the Data --- 2. Distance and Kernel Functions\n#### 3. Learning Algorithm for Binary Classification & One versus all --- 4. Confusion Matrix Results for C = 10\n#### 5. Evaluation over C = [ 0.1, 1, 10, 100, 1000] --- 6. Plotting Accuracy Scores per C","metadata":{}},{"cell_type":"code","source":"# Onur Can \n# Project is done for Prof. Mehmet Gönen's DASC 521: Introduction to Machine Learning @ Koç University MSc Data Science Program\n# Thanks Prof Mehmet for the dataset generation and instructions\n\nimport cvxopt as cvx    # for solver \nimport numpy as np      # for matrix operations\nimport pandas as pd     # for confusion matrix\nimport matplotlib.pyplot as plt     # for Plotting\nimport scipy.spatial.distance as dt # for Euclidean distance calculation","metadata":{"execution":{"iopub.status.busy":"2022-06-06T18:40:31.967516Z","iopub.execute_input":"2022-06-06T18:40:31.969559Z","iopub.status.idle":"2022-06-06T18:40:32.068302Z","shell.execute_reply.started":"2022-06-06T18:40:31.969444Z","shell.execute_reply":"2022-06-06T18:40:32.067115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1. Importing and Processing the Data","metadata":{}},{"cell_type":"code","source":"# Reading the data\ndataset = np.genfromtxt(\"../input/image-Classification/images.csv\", delimiter=\",\")\nlabels = np.genfromtxt(\"../input/image-Classification/labels.csv\", delimiter=\",\")\n\n# Processing the train & test data\nX_train = dataset[0:1000,:]\ny_train = labels[0:1000].astype(int)\nx_test = dataset[1000:5000,:]\ny_test = labels[1000:5000].astype(int)\n\n# Get number of samples and number of features\nN_train = len(y_train)   # Train Data Size\nN_test = len(y_test)     # Test Data Size\nD_train = X_train.shape[1]  # Number of Dimensions\nK = np.max(y_train)         # Number of Classes\n\nprint(X_train.shape, y_train.shape, x_test.shape, y_test.shape, N_train, D_train, K)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T18:40:32.069176Z","iopub.status.idle":"2022-06-06T18:40:32.069913Z","shell.execute_reply.started":"2022-06-06T18:40:32.069722Z","shell.execute_reply":"2022-06-06T18:40:32.069743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Distance and Kernel Functions","metadata":{}},{"cell_type":"markdown","source":"$\\begin{equation}\n    \\begin{split}\n        d(\\boldsymbol{x}_{i}, \\boldsymbol{x}_{j}) &= ||\\boldsymbol{x}_{i} - \\boldsymbol{x}_{j}||_{2} = \\sqrt{(\\boldsymbol{x}_{i} - \\boldsymbol{x}_{j})^{\\top} (\\boldsymbol{x}_{i} - \\boldsymbol{x}_{j})} = \\sqrt{\\sum\\limits_{d = 1}^{D}(x_{id} - x_{jd})^{2}} \\\\\n        k(\\boldsymbol{x}_{i}, \\boldsymbol{x}_{j}) &= \\exp\\left(-\\dfrac{||\\boldsymbol{x}_{i} -\\boldsymbol{x}_{j}||_{2}^{2}}{2s^{2}}\\right)\n    \\end{split}\n\\end{equation}$","metadata":{}},{"cell_type":"code","source":"# define Gaussian kernel function\ndef gaussian_kernel(X1, X2, s):\n    # Takes X1 and X2 points & calculates the Kernel Gaussian defined above with given s\n    D = dt.cdist(X1, X2)   # Euclidian Distance Calculation from Scipy Library\n    K = np.exp(-D**2 / (2 * s**2)) # Kernel Function\n    return(K)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T18:40:32.070885Z","iopub.status.idle":"2022-06-06T18:40:32.071367Z","shell.execute_reply.started":"2022-06-06T18:40:32.071197Z","shell.execute_reply":"2022-06-06T18:40:32.071216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Learning Algorithm for Binary Classification & One versus all\n#### Func. \"one_versus_all\"  takes datapoints, C & s parameters and comparison class\n#### Outputs one_versus_all scores for training & test class for each data point\n#### Note: Solver function cvx.solvers.qp messages was silented.","metadata":{}},{"cell_type":"code","source":"def one_versus_all(x, y, S, c, class_to_all, test_to_all = []):\n    # Initialization\n    s = S                 # Kernel width\n    SVM_K = class_to_all  # Class to compare versus all\n    X_compare = x         # X data points\n    y_compare = y         # Labels\n    C = c                 # Regularization parameter\n    epsilon = 1e-3        # Quadratic programming solver parameter\n    test_predicted = None # Evaluation of new data points\n    \n    # This is the part of where we change labels to 1, -1 and apply Kernel & Ko[Y.Y^T]\n    y_train_modified = np.where(y_compare == SVM_K , 1, -1)\n    K_train = gaussian_kernel(X_compare, X_compare, s)\n    yyK = np.matmul(y_train_modified[:,None], y_train_modified[None,:]) * K_train #y_train shape correction\n\n    # CSX solver parameter initializations / Changing conditions to Matrix Form\n    P = cvx.matrix(yyK)                     # Already calculated before\n    q = cvx.matrix(-np.ones((N_train, 1)))  # Column vector of ones\n    # Representing -a <= 0  &  a <=  C constraints respectively in G & h\n    G = cvx.matrix(np.vstack((-np.eye(N_train), np.eye(N_train))))\n    h = cvx.matrix(np.vstack((np.zeros((N_train, 1)), C * np.ones((N_train, 1)))))\n    A = cvx.matrix(1.0 * y_train_modified[None,:])  # y_transpose\n    b = cvx.matrix(0.0)  # equal to 0\n    \n    cvx.solvers.options['show_progress'] = False    # to silent solver messages\n    result = cvx.solvers.qp(P, q, G, h, A, b)      \n    alpha = np.reshape(result[\"x\"], N_train)        # alpha values for given X data_set\n    alpha[alpha < C * epsilon] = 0                  # makes 0.00000000000001 to 0\n    alpha[alpha > C * (1 - epsilon)] = C            # makes 9.99999999999999 to C\n    support_indices, = np.where(alpha != 0)\n    active_indices, = np.where(np.logical_and(alpha != 0, alpha < C))\n    \n    # W0 parameter calculation\n    w0 = np.mean(y_train_modified[active_indices] * (1 - np.matmul(yyK[np.ix_(\n        active_indices, support_indices)], alpha[support_indices])))\n    \n    # Binary Classification for train_set for given one versus all structure\n    train_predicted = np.matmul(K_train, y_train_modified[:,None] * alpha[:,None]) + w0\n    \n    # If given new test set calculate SVM scores\n    if len(test_to_all) != 0:\n        K_test = gaussian_kernel(test_to_all, X_compare, s)\n        test_predicted = np.matmul(K_test, y_train_modified[:,None] * alpha[:,None]) + w0\n    \n    return train_predicted, test_predicted","metadata":{"execution":{"iopub.status.busy":"2022-06-06T18:40:32.072246Z","iopub.status.idle":"2022-06-06T18:40:32.072714Z","shell.execute_reply.started":"2022-06-06T18:40:32.072531Z","shell.execute_reply":"2022-06-06T18:40:32.072549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Confusion Matrix Results for C = 10","metadata":{}},{"cell_type":"code","source":"# Calculate SVMs on training & test samples for C = 10\nSVM_train_predicted = np.empty((K , X_train.shape[0]))\nSVM_test_predicted = np.empty((K , x_test.shape[0]))\n# Save results from each one-versus-all comparison to take Maximum\nfor klass in range(5):\n    train_scores, test_scores = one_versus_all(X_train, y_train, 10, 10, klass+1, x_test)\n    SVM_train_predicted[klass] = np.transpose(train_scores)\n    SVM_test_predicted[klass] = np.transpose(test_scores)\n\nprint(\"------------Training Data Predictions Shape and Confusion Matrix ------------\")\nprint(SVM_train_predicted.shape)\ntrain_y_predicted = np.argmax(SVM_train_predicted, axis = 0) + 1 # take MAX of the scores \ntrain_confusion_matrix = pd.crosstab(np.reshape(train_y_predicted, N_train), y_train, rownames = ['y_predicted'], colnames = ['y_train'])\nprint(train_confusion_matrix)\nprint(\"\\n------------Test Data Predictions Shape and Confusion Matrix ------------\")\nprint(SVM_test_predicted.shape)\ntest_y_predicted = np.argmax(SVM_test_predicted, axis = 0) + 1 # take MAX of the scores\ntest_confusion_matrix = pd.crosstab(np.reshape(test_y_predicted, N_test), y_test, rownames = ['y_predicted'], colnames = ['y_train'])\nprint(test_confusion_matrix)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-06T18:40:32.073567Z","iopub.status.idle":"2022-06-06T18:40:32.074045Z","shell.execute_reply.started":"2022-06-06T18:40:32.073866Z","shell.execute_reply":"2022-06-06T18:40:32.073884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. Evaluation over C = [ 0.1, 1, 10, 100, 1000]","metadata":{}},{"cell_type":"code","source":"C_values = [10**-1, 10**0, 10**1, 10**2, 10**3 ]\ntrain_accuracy_values = np.empty(len(C_values))\ntest_accuracy_values = np.empty(len(C_values))\n\n# For each different C value apply One_versus_all classification for training / test\nfor i in range(len(C_values)):\n    SVM_train_predicted = np.empty((K , X_train.shape[0]))\n    SVM_test_predicted = np.empty((K , x_test.shape[0]))\n    for klass in range(5):\n        train_scores, test_scores = one_versus_all(X_train, y_train, 10 , C_values[i] , klass+1, x_test)\n        SVM_train_predicted[klass] = np.transpose(train_scores)\n        SVM_test_predicted[klass] = np.transpose(test_scores)    \n    train_y_predicted = np.argmax(SVM_train_predicted, axis = 0) + 1\n    test_y_predicted = np.argmax(SVM_test_predicted, axis = 0) + 1\n    train_accuracy = np.sum(np.array([y_train == train_y_predicted ]) * 1) / N_train\n    test_accuracy = np.sum(np.array([y_test == test_y_predicted ]) * 1) / N_test\n    train_accuracy_values[i] = train_accuracy\n    test_accuracy_values[i] = test_accuracy\n\nC_values_axis = np.log10(C_values) # log10 was used to fit the accuracy line in x_axis in plot\nprint(\"-----Train Accuracy per C-----\\n\", train_accuracy_values)\nprint(\"-----Test Accuracy per C-----\\n\", test_accuracy_values)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-06T18:40:32.179852Z","iopub.execute_input":"2022-06-06T18:40:32.180332Z","iopub.status.idle":"2022-06-06T18:40:32.206452Z","shell.execute_reply.started":"2022-06-06T18:40:32.180286Z","shell.execute_reply":"2022-06-06T18:40:32.204764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6. Plotting Accuracy Scores per C","metadata":{}},{"cell_type":"code","source":"# Plotting Accuracy/C\n# C_values_axis = log10 of C values to fit X_axis\nplt.figure(figsize = (10,8))\nplt.plot(C_values_axis, train_accuracy_values, \"b.-\", label = \"training\", markersize= 12)\nplt.plot(C_values_axis, test_accuracy_values, \"r.-\", label = \"test\", markersize= 12)\nplt.xticks(C_values_axis, [\"10^(-1)\", \"10^(0)\", \"10^(1)\", \"10^(2)\", \"10^(3)\"])\nplt.xlabel(\"Regularization Parameter (C)\")\nplt.ylabel(\"Accuracy\")\nplt.legend(loc=\"upper left\")\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T18:40:32.207942Z","iopub.status.idle":"2022-06-06T18:40:32.208882Z","shell.execute_reply.started":"2022-06-06T18:40:32.208593Z","shell.execute_reply":"2022-06-06T18:40:32.208627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}