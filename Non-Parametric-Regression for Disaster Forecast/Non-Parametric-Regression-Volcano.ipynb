{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Onur Can \n# Project is done for Prof. Mehmet Gönen's DASC 521: Introduction to Machine Learning @ Koç University MSc Data Science Program\n# Thanks Prof Mehmet for the dataset generation and instructions\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math as math","metadata":{"execution":{"iopub.status.busy":"2022-06-05T15:11:04.617850Z","iopub.execute_input":"2022-06-05T15:11:04.618223Z","iopub.status.idle":"2022-06-05T15:11:04.622404Z","shell.execute_reply.started":"2022-06-05T15:11:04.618193Z","shell.execute_reply":"2022-06-05T15:11:04.621749Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# 0. Data Preparation","metadata":{}},{"cell_type":"code","source":"#importing data_set\ndata_set = np.genfromtxt(\"../input/volcan-dataset/volcano_data_set.csv\", delimiter = \",\", skip_header = 1)\n\ntraining_set = data_set[0:150,:]\ntest_set = data_set[150:,:]\nprint(training_set.shape)\nprint(test_set.shape)\n\n#Splitting and preparing the data\nx_train = training_set[:,0]\ny_train = training_set[:,1].astype(int)\nx_test = test_set[:,0]\ny_test = test_set[:,1].astype(int)\nN_test = test_set.shape[0]\n\nprint(x_train.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T15:11:04.635605Z","iopub.execute_input":"2022-06-05T15:11:04.636182Z","iopub.status.idle":"2022-06-05T15:11:04.651265Z","shell.execute_reply.started":"2022-06-05T15:11:04.636149Z","shell.execute_reply":"2022-06-05T15:11:04.650618Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# 1. Regressogram\n### 1.1. Calculating G_hat for training data","metadata":{}},{"cell_type":"code","source":"bin_width = 0.37\norigin = +1.5\nend_point = +5.2  #this is for finishing origin & Next bin will have 0 data\n\n#--------------------------------------------------------------------\n#Basic concept: Define bins with origin and width - put x_trains in these take AVERAGE of their\n#respective R values that is your y_predicted for new_x if it is in that interval\n# g = sum( b(x,x_t)) * r_t / sum( b(x,x_t))   where b(x,x_t) is 1 if x_t is \n#in same bin with x (One function)\n#--------------------------------------------------------------------\n\n#Bin Settings\nleft_borders = np.arange(origin, end_point, bin_width)\nright_borders = np.arange(origin + bin_width, end_point + bin_width, bin_width)\nprint(left_borders)\nprint(right_borders)\nprint(\"Number of Bins = \", len(left_borders))\n\n#Calculating Denominator of G_hat\nBin_Number_of_observations = np.asarray([np.sum((left_borders[b] < x_train) & (\n    x_train <= right_borders[b])) for b in range(len(left_borders))])\nprint(\"\\nNumber of elements in each Bin from left to right\")\nprint(Bin_Number_of_observations)\n\n#Calculating nominator of G_hat\nBin_R_Values_Sum = np.zeros(Bin_Number_of_observations.shape[0])\nfor i in range(len(left_borders)):\n    Bin_R_Values_Sum[i] = np.sum(y_train[(left_borders[i] < x_train) & (\n    x_train <= right_borders[i])])\nprint(\"\\nSum of R values in each Bin from left to right\")\nprint(Bin_R_Values_Sum)\n\n#Calculating G_hat values\ng_hat = Bin_R_Values_Sum / Bin_Number_of_observations\nprint(\"\\nG_hat values for each Bin from left to right\")\nprint(g_hat)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-05T15:11:04.652565Z","iopub.execute_input":"2022-06-05T15:11:04.652996Z","iopub.status.idle":"2022-06-05T15:11:04.666715Z","shell.execute_reply.started":"2022-06-05T15:11:04.652968Z","shell.execute_reply":"2022-06-05T15:11:04.665828Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### 1.2. Plotting Training & Test Data and Regressogram","metadata":{}},{"cell_type":"code","source":"#Plotting all the fields in the homework\nplt.figure(figsize = (12,6))\n#training set\nplt.plot(x_train, y_train, \"b.\", markersize = 10, label = \"training\")\n#test set\nplt.plot(x_test, y_test, \"r.\", markersize = 10, label = \"test\")\n#Plotting regressogram\nfor j in range(len(left_borders)):\n    plt.plot([left_borders[j], right_borders[j]], [g_hat[j], g_hat[j]], \"k-\")\nfor j in range(len(left_borders) - 1):\n    plt.plot([right_borders[j], right_borders[j]], [g_hat[j], g_hat[j + 1]], \"k-\")\nplt.xlabel(\"Eruption Time (min)\")\nplt.ylabel(\"Waiting time to next eruption (min)\")\nplt.legend(loc=\"upper left\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T15:11:04.668258Z","iopub.execute_input":"2022-06-05T15:11:04.669005Z","iopub.status.idle":"2022-06-05T15:11:04.922757Z","shell.execute_reply.started":"2022-06-05T15:11:04.668965Z","shell.execute_reply":"2022-06-05T15:11:04.921971Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### 1.3. Regressogram Root Mean Squared Error (RMSE)\n","metadata":{}},{"cell_type":"code","source":"#Calculating predicted values for each test data point\ny_predicted = np.zeros(x_test.shape[0])\n\n#Finding the respective bin for test data points therefore corresponding y_predicted\nfor i in range(len(x_test)):\n    for j in range(len(left_borders)):\n        if x_test[i] > left_borders[j]:\n            y_predicted[i] = g_hat[j]\n\n#Root Mean Squared Error Formula\nRMSE = np.sqrt(np.sum(((y_predicted - y_test)**2)) / N_test)\nprint(\"----------------------------------------------------------\")\nprint(\"Regressogram => RMSE is \", RMSE ,\" when h is \", bin_width)\nprint(\"----------------------------------------------------------\")\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-05T15:11:04.924696Z","iopub.execute_input":"2022-06-05T15:11:04.925078Z","iopub.status.idle":"2022-06-05T15:11:04.932865Z","shell.execute_reply.started":"2022-06-05T15:11:04.925048Z","shell.execute_reply":"2022-06-05T15:11:04.932124Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# 2. Running Mean Smoother\n### 2.1 Calculating G_hat for Traning Set","metadata":{}},{"cell_type":"code","source":"end_point = +5.2\ndata_interval = np.linspace(origin, end_point, 371 )                            \nbin_width = 0.37\n\n#--------------------------------------------------------------------\n#Basic Concept : Instead of defining origin/fixed bins - you define a symetric interval around X point\n#take the AVERAGE values of their R values\n#g^(x) = Sum(w(u)*r_t / Sum (w(u)) where u = (x-x_t)/h and ) .... w(u) is 1 if abs(w(u)) < 1 \n#--------------------------------------------------------------------\n\n#For Running Mean Smoother for training data\n#Calculating Denominator of G_hat\nBin_Number_of_observations = np.asarray([np.sum(( x - 0.5 * bin_width < x_train) & (\n   x_train <=  x + 0.5 * bin_width)) for x in data_interval])\n\n#Calculating nominator of G_hat\nBin_R_Values_Sum = np.zeros(Bin_Number_of_observations.shape[0])\nfor i in range(Bin_Number_of_observations.shape[0]):\n    Bin_R_Values_Sum[i] = np.sum(y_train[( data_interval[i] - 0.5 * bin_width < x_train) & (\n   x_train <=  data_interval[i] + 0.5 * bin_width)])\n\ng_hat = Bin_R_Values_Sum / Bin_Number_of_observations\n","metadata":{"execution":{"iopub.status.busy":"2022-06-05T15:11:04.933853Z","iopub.execute_input":"2022-06-05T15:11:04.934423Z","iopub.status.idle":"2022-06-05T15:11:04.954891Z","shell.execute_reply.started":"2022-06-05T15:11:04.934390Z","shell.execute_reply":"2022-06-05T15:11:04.954167Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### 2.2 Running Mean Smoother Root Mean Squared Error (RMSE)\n","metadata":{}},{"cell_type":"code","source":"#Calculating predicted values for each test data point\ny_predicted = np.zeros(x_test.shape[0])\n\n#Finding the respective bin for test data points therefore corresponding y_predicted\nNumber_of_observations = np.asarray([np.sum(( x - 0.5 * bin_width < x_train) & (\n   x_train <=  x + 0.5 * bin_width)) for x in x_test])\n\nBin_Values_Sum = np.asarray([np.sum(y_train[( x - 0.5 * bin_width < x_train) & (\n   x_train <=  x + 0.5 * bin_width)]) for x in x_test])\n\ny_predicted = Bin_Values_Sum / Number_of_observations\n#Root Mean Squared Error Formula\nRMSE = np.sqrt(np.sum(((y_predicted - y_test)**2)) / N_test)\n\nprint(\"-----------------------------------------------------------------------\")\nprint(\"Running Mean Smoother => RMSE is \", RMSE ,\" when h is \", bin_width)\nprint(\"-----------------------------------------------------------------------\")","metadata":{"execution":{"iopub.status.busy":"2022-06-05T15:11:04.956429Z","iopub.execute_input":"2022-06-05T15:11:04.957084Z","iopub.status.idle":"2022-06-05T15:11:04.969118Z","shell.execute_reply.started":"2022-06-05T15:11:04.957044Z","shell.execute_reply":"2022-06-05T15:11:04.968163Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### 2.3 Plotting Training & Test Data and Running Mean Smoother Line","metadata":{}},{"cell_type":"code","source":"#Plotting all the fields in the homework\nplt.figure(figsize = (12,6))\n#training set\nplt.plot(x_train, y_train, \"b.\", markersize = 10, label = \"training\")\n#test set\nplt.plot(x_test, y_test, \"r.\", markersize = 10, label = \"test\")\n#Plotting running mean smooth\nplt.plot(data_interval, g_hat, \"k-\")\nplt.xlabel(\"Eruption Time (min)\")\nplt.ylabel(\"Waiting time to next eruption (min)\")\nplt.legend(loc=\"upper left\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T15:11:04.970358Z","iopub.execute_input":"2022-06-05T15:11:04.970755Z","iopub.status.idle":"2022-06-05T15:11:05.174182Z","shell.execute_reply.started":"2022-06-05T15:11:04.970716Z","shell.execute_reply":"2022-06-05T15:11:05.173310Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# 3. Kernel Smoother\n### 3.1 Calculating G_hat for Traning Set","metadata":{}},{"cell_type":"code","source":"#Parameters for G_Hat Calculation\nh = +0.37\nend_point = +5.2\norigin = +1.5\ndata_interval = np.linspace(origin, end_point, 371)\nKernel_Line = np.zeros(371)\n\n#--------------------------------------------------------------------\n#Basic Concept: For any new X all traning data points have an effect on that point\n#with decreasing effect as /x-xi/ .. Putting less weights to further points .. Also continuos\n#User Gaussian Kernel for this K(u) = 1/sqrt(2pie) * exp(-u^2 / 2) where u = (x-xi)/h\n#g^(x) = Sum(K(u)*r_t / Sum (K(u))\n#--------------------------------------------------------------------\n\nfor x in range(len(data_interval)):\n    #denominator of g_hat\n    Kernel_estimation = np.asarray([ (1 / np.sqrt(2 * math.pi) * np.exp(\n        -0.5 * ( data_interval[x] - i )**2 / h**2 )) for i in x_train])\n    #nominator of g_hat\n    r_values = Kernel_estimation * y_train\n    #g_hat values\n    Kernel_Line[x] = np.sum(r_values) / np.sum(Kernel_estimation)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-05T15:11:05.175452Z","iopub.execute_input":"2022-06-05T15:11:05.175713Z","iopub.status.idle":"2022-06-05T15:11:05.510558Z","shell.execute_reply.started":"2022-06-05T15:11:05.175689Z","shell.execute_reply":"2022-06-05T15:11:05.509765Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### 3.2 Plotting Training & Test Data and Kernel Smoother Line","metadata":{}},{"cell_type":"code","source":"#Plotting all the fields in the homework\nplt.figure(figsize = (12,6))\n#training set\nplt.plot(x_train, y_train, \"b.\", markersize = 10, label = \"training\")\n#test set\nplt.plot(x_test, y_test, \"r.\", markersize = 10, label = \"test\")\n#Plotting running mean smooth\nplt.plot(data_interval, Kernel_Line, \"k-\")\nplt.xlabel(\"Eruption Time (min)\")\nplt.ylabel(\"Waiting time to next eruption (min)\")\nplt.legend(loc=\"upper left\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T15:11:05.511559Z","iopub.execute_input":"2022-06-05T15:11:05.511846Z","iopub.status.idle":"2022-06-05T15:11:05.715634Z","shell.execute_reply.started":"2022-06-05T15:11:05.511820Z","shell.execute_reply":"2022-06-05T15:11:05.714888Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### 3.3 Kernel Smoother Root Mean Squared Error (RMSE)","metadata":{}},{"cell_type":"code","source":"#Calculating predicted values for each test data point\ny_predicted = np.zeros(x_test.shape[0])\n\nfor x in range(len(x_test)):\n    #denominator of g_hat\n    Kernel_estimation = np.asarray([ (1 / np.sqrt(2 * math.pi) * np.exp(\n        -0.5 * ( x_test[x] - i )**2 / h**2 )) for i in x_train])\n    #nominator of g_hat\n    r_values = Kernel_estimation * y_train\n    #g_hat values\n    y_predicted[x] = np.sum(r_values) / np.sum(Kernel_estimation)\n\n#RMSE Formulation    \nRMSE = np.sqrt(np.sum(((y_predicted - y_test)**2)) / N_test)\n\nprint(\"-----------------------------------------------------------------------\")\nprint(\"Kernel Mean Smoother => RMSE is \", RMSE ,\" when h is \", bin_width)\nprint(\"-----------------------------------------------------------------------\")","metadata":{"execution":{"iopub.status.busy":"2022-06-05T15:11:05.717374Z","iopub.execute_input":"2022-06-05T15:11:05.717700Z","iopub.status.idle":"2022-06-05T15:11:05.823351Z","shell.execute_reply.started":"2022-06-05T15:11:05.717673Z","shell.execute_reply":"2022-06-05T15:11:05.822416Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}